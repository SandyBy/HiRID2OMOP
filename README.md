# HiRID2OMOP

This repository contains the whole tools and documents to perform an Extract-Transform-Load (ETL) process for mapping the
[HiRID database](https://physionet.org/content/hirid/1.0/) to the [OMOP Common Data Base Model v6.0](https://github.com/OHDSI/CommonDataModel).
The process involved the transformation and creation of OMOP CDM variables based on the HiRID elements.

# The repository

The repository contains the next elements:

 * HiRID_2_CDM.ipynb: The Jupyter notebook is the implementation of the ETL process. It only needs the root directory where the HiRID csv files are located to generate the OMOP CDM tables based on the ETL document.
 * ETL Folder: Folder where the ETL documents needed for the OHDSI tools
   * Rabbit In A Hat Files: This folder contains the save files for Rabbit in a Hat. You can download RabbitInAHat [here](https://www.ohdsi.org/analytic-tools/whiterabbit-for-etl-design/)
   * UsagiFiles: This folder contains the HiRID vocabulary mapping files to OMOP CDM. To use it, you need to download from [Athena](https://athena.ohdsi.org/search-terms/start) these vocabularies.
	  * SNOMED
	  * LOINC
	  * RxNorm
	  * OMOP Gender
	  * RxNorm Extension
	  * OMOP Extension
   * Vocabulary Mapping: The vocabulary mapping from HiRID to OMOP. This file is used to generate the content of OMOP's table source_to_concept_map
 * docs: Folder that contains the ETL document generated by RabbitInAHat and a PDF with the resume of all the work done at the translation
 * scripts: Contains the query to upload the vocabulary at the OMOP cdm and clean tables

# How to run the Translation Process

1. Download HiRID files, decompress it but do not change the directory order.
2. Create the OMOP CDM database, the [instructions](https://github.com/OHDSI/CommonDataModel/tree/master/PostgreSQL) are at the github repository 
3. Open the Jupyter notebook, make sure to have the requiered libraries at the notebook
4. Follow carefully the instructions at the jupyter notebook
5. Once the jupyter query is executed correctly, run the script 'OMOP2HiRID_vocab_translation_query.sql' at the scripts folder

**Note**: As an alternative, you can create an empty database with all the keys included and then run the jupyter notebook. In case you need to insert data again, run the script 'OMOP2HiRID_Management_queries.sql' in the **scripts** folder

